[
  {
    "objectID": "docs/reproducibility-guidance/how-to-cite-digital-objects.html",
    "href": "docs/reproducibility-guidance/how-to-cite-digital-objects.html",
    "title": "How to reference digital objects in papers",
    "section": "",
    "text": "When writing a paper, researchers have several options in how to cite their research compendium, open data, as well as code they used."
  },
  {
    "objectID": "docs/reproducibility-guidance/how-to-cite-digital-objects.html#overall",
    "href": "docs/reproducibility-guidance/how-to-cite-digital-objects.html#overall",
    "title": "How to reference digital objects in papers",
    "section": "Overall",
    "text": "Overall\nWhile researchers can reference and describe the data and code they use throughout their code in any number of ways that best fits the layout of the research paper, we recommend a simple process for citation depending on if there is a DOI for the data and code.\n\nReference the research compendium and data made available in a structured way\nFirst off, we recommend the structure used in Nature by the FAIR team in their Baker et al (2022) paper. Specifically, at the end of the paper just before the bibiliography, two sections are added on the data and the code available with this research. Adopting this structured approach is useful to quickly show readers how to find the various objects that are published alongside the paper.\nA research compendium can be published to GitHub or on a repository (in a way that aligns with the requirements of the institution the researcher works at) and cited in the “Code availability” section. Datasets made available by this research project can be published on a data repository (ideally) and referenced in the “Data availability” section.\n\n\nReference datasets and code in the bibliography\nOthers’ code and datasets used as input to the research should be referenced in the bibliography/references section. Ideally both have a DOI so the object is easily findable and citable. The following two articles go into more details on how to cite and data as there are some nuances that may make it challenging."
  },
  {
    "objectID": "docs/reproducibility-guidance/citing-data.html",
    "href": "docs/reproducibility-guidance/citing-data.html",
    "title": "How to cite open data",
    "section": "",
    "text": "Citing an open dataset for a research project is straightforward if it is archived in an open research repository like Zenodo. In this case it will have the relevant metadata, along with a DOI, to make it easily citeable, even if access to these data is restricted. Unfortunately open datasets do not always reside in a repository like Zenodo. A common place for open datasets in statistics, and in particular price statistics, is as part of an R package on CRAN. Citing these datasets is not difficult—all CRAN packages have a DOI and are easily cited—but, as part of a package, data are more difficult to access."
  },
  {
    "objectID": "docs/reproducibility-guidance/citing-data.html#example-priceindices",
    "href": "docs/reproducibility-guidance/citing-data.html#example-priceindices",
    "title": "How to cite open data",
    "section": "Example: {PriceIndices}",
    "text": "Example: {PriceIndices}\nThe {PriceIndices} R package comes with several datasets that can be useful for comparing different index-number methods. Citing these data is easy because R packages are highly citeable.\n@Manual{,\n  title = {PriceIndices: Calculating Bilateral and Multilateral Price Indexes},\n  author = {Jacek Białek},\n  year = {2025},\n  doi = {10.32614/CRAN.package.PriceIndices},\n  url = {https://cran.r-project.org/package=PriceIndices},\n  note = {R package version 0.2.3}\n}\nUsing these data is easy enough with R. Simply install the package and the datasets become available from within R."
  },
  {
    "objectID": "docs/datasets-guidance/opening-up-datasets.html",
    "href": "docs/datasets-guidance/opening-up-datasets.html",
    "title": "Options for making a dataset openly available",
    "section": "",
    "text": "Do you have a dataset that you would like to register on the data catalogue? Follow the simple.\nThe project does not have final guidance on the topic. For now, we recommend the following process:\n\nPublish your dataset in a data repository that mints a Digital Object Identifier (DOI). This will make your dataset citable by other researchers.\nRegister your dataset in the price statistics data catalogue. See the contributing guidance of the data catalogue for more information.\n\nWe would like to hear of other datasets that may not fit this process (such as datasets that are not registered in an data repository like Zenodo). We are happy to work with you to get the dataset there.\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/about.html",
    "href": "docs/about.html",
    "title": "About this project",
    "section": "",
    "text": "This project is led by Workstream 5 of the UN Task Team on Scanner data, part of the UN Committee of Experts on Big Data and Data Science for Official Statistics (UN-CEBD)\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Welcome to the Price Statistics Reproducibility Project",
    "section": "",
    "text": "Welcome to the project aiming to help researchers in the field of price statistics make their projects more reproducible and to use more open (and FAIR) data in their projects."
  },
  {
    "objectID": "docs/index.html#what-this-project-aims-to-do",
    "href": "docs/index.html#what-this-project-aims-to-do",
    "title": "Welcome to the Price Statistics Reproducibility Project",
    "section": "What this project aims to do",
    "text": "What this project aims to do\nResearch in the price statistics discipline is not as reproducible as it could be. Most researchers utilize proprietary datasets and don’t publish the code alongside the research so that a specific methodology or finding can be easily reproduced. The idea of this project is to help in both topics! We aim to:\n\nProvide clear and approachable guidance on how researchers in price statistics can make their projects reproducible (including by learning new skills, by understanding how to publish and cite code, and many other topics)\nSupport cataloging open datasets that can be used as benchmarks to use for research purposes. The idea is to have all findings trialed and demonstrated on open benchmark datasets."
  },
  {
    "objectID": "docs/index.html#what-this-project-does-not-aim-to-do",
    "href": "docs/index.html#what-this-project-does-not-aim-to-do",
    "title": "Welcome to the Price Statistics Reproducibility Project",
    "section": "What this project does not aim to do",
    "text": "What this project does not aim to do\nAs there is a lot of great guidance on the topic already, including The Turing Way, Reproducible Analytical Pipelines or RAP, and many more resources - the idea is to distilling key information for the price statistics community, not to create a new standard."
  },
  {
    "objectID": "docs/index.html#key-concepts",
    "href": "docs/index.html#key-concepts",
    "title": "Welcome to the Price Statistics Reproducibility Project",
    "section": "Key concepts",
    "text": "Key concepts\nThe idea is to make it easy for researchers to prioritize using open data and publish their research compendium (code and related objects) with their research. In either case, other researchers can reproduce the research, expand on it, and apply it to internal data.\n\n\n\nFigure 1: Shifting from limited to fuller reproducibility (summarizing the main materials for a research, inspired by Bontemps and Orozco (2020))\n\n\nStepping back a bit, let us first define what reproducibility means. While there are many definitions, the aim of reproducible research is to ensure that work can be independently recreated from the same data and the same code. If researchers don’t have access to the same data (which may be locked down due to confidentiality reasons), researchers can aim to replicate the result with their own data.\n\n\n\nFigure 2: Definitions of Reproducible Research by the Turing Way\n\n\nTo support open-science within price statistics, the project recommends:\n\nThat researchers in the discipline use of open datasets when available in order to support reproducible or robust research. This is why the project has started a catalogue of open datasets that can be leveraged for research purposes;\nThe publication of research code and related research artifacts so that other researchers can independently reproduce all research results. The project provides guidance on structuring and publishing research code so that others can reproduce your results.\n\nIdeally, both aspects are adopted by researchers. However even the adoption of one is valuable as it is possible to replicate the results with different data. Without either, a slower and less effective goal of science in the discipline is generalizable findings by non-reproducible research simply from what is stated in each researchers’ paper (the status quo)."
  },
  {
    "objectID": "docs/index.html#navigating-the-guidance",
    "href": "docs/index.html#navigating-the-guidance",
    "title": "Welcome to the Price Statistics Reproducibility Project",
    "section": "Navigating the guidance",
    "text": "Navigating the guidance\nWhile guidance will expand over time, the following sections and links can help researchers in the discipline:\n\nCatalogue of open datasets available to price statisticians\nGuidance on the open data catalogue and how to register new datasets\nRecommendations on making research code open\nGuidance on how to cite others’ code and how to cite open datasets in your research.\nOptions for those wanting to publish new datasets in the catalogue\n\nWe also recommend other guidance related to the topic but not specific to price statistics:\n\nThe Turing Way is a wonderful and comprehensive guide for open research\nThe Reproducible Analytical Pipelines (or RAP) is another very applicable paradigm for reproducibility, although has come to be applied within official statistics as a way to operationalize production processes. Many UN classes are nowadays teaching these principles as a basic minimum standard within official statistics and have been recommended by the ONS for price statistics.12"
  },
  {
    "objectID": "docs/index.html#footnotes",
    "href": "docs/index.html#footnotes",
    "title": "Welcome to the Price Statistics Reproducibility Project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor instance see class on RAP by UN ESCAP, by UN SIAP for classes.↩︎\nSee Price and Marques (2023) for an overview of RAP to price statistics.↩︎"
  },
  {
    "objectID": "project-content/other-open-materials.html",
    "href": "project-content/other-open-materials.html",
    "title": "List of open materials relevant to price statistics",
    "section": "",
    "text": "The following list is a mini-compenidum of other materials that may be of interest to reseachers in the prices statistics discipline. The list is meant to be live and up to date, hence if you see any inaccuracies, a broken link, or something missing - please feel free to add!\n\n\n\nAwesome list of official statistics software\nReferences in JOSS paper\nSteve’s stars\n\n\n\n\nhicp package (Eurostat) - provides functions to access and work with HICP data (price indices and weights) from Eurostat’s public database\n‘PriceIndices’ package\nIndexNumR package\npiar package\nCRAN task view for official statistics\n\n\n\n\n\nPriceIndexCalc package - a partially working package to calculate price indices.\n\n\n\n\n\ndff - a repo on Dominick’s Finer Foods showcasing the dataset by Jens Mehrhoff\n\n\n\n\n\n\n\n\nThe Turing Way - an open science/collaboration project\nreproducible-project-template - repo template (i.e cookie cutter template)\n\n\n\n\n\nRAP Companion, by Matthew Gregory and Matthew Upson shows much of the original RAP ideas (for R)\nUdemy course on RAP with R\nReproducible Analytical Pipelines or RAP (NHS) site\nESCAP training on RAP with web scraping application for price statistics"
  },
  {
    "objectID": "project-content/other-open-materials.html#packages-applicable-to-price-statistics",
    "href": "project-content/other-open-materials.html#packages-applicable-to-price-statistics",
    "title": "List of open materials relevant to price statistics",
    "section": "",
    "text": "Awesome list of official statistics software\nReferences in JOSS paper\nSteve’s stars\n\n\n\n\nhicp package (Eurostat) - provides functions to access and work with HICP data (price indices and weights) from Eurostat’s public database\n‘PriceIndices’ package\nIndexNumR package\npiar package\nCRAN task view for official statistics\n\n\n\n\n\nPriceIndexCalc package - a partially working package to calculate price indices.\n\n\n\n\n\ndff - a repo on Dominick’s Finer Foods showcasing the dataset by Jens Mehrhoff"
  },
  {
    "objectID": "project-content/other-open-materials.html#reproducibility-resources",
    "href": "project-content/other-open-materials.html#reproducibility-resources",
    "title": "List of open materials relevant to price statistics",
    "section": "",
    "text": "The Turing Way - an open science/collaboration project\nreproducible-project-template - repo template (i.e cookie cutter template)\n\n\n\n\n\nRAP Companion, by Matthew Gregory and Matthew Upson shows much of the original RAP ideas (for R)\nUdemy course on RAP with R\nReproducible Analytical Pipelines or RAP (NHS) site\nESCAP training on RAP with web scraping application for price statistics"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "Code of Conduct\nWe are a community based on openness, as well as friendly and didactic discussions.\nWe aspire to treat everybody equally, and value their contributions.\nDecisions are made based on technical merit and consensus.\nCode is not the only way to help the project. Reviewing pull requests, answering questions to help others on mailing lists or issues, organizing and teaching tutorials, working on the website, improving the documentation, are all priceless contributions.\nWe abide by the principles of openness, respect, and consideration of others of the Python Software Foundation: https://www.python.org/psf/codeofconduct/\n\n\n\n\n Back to top"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "When contributing, post comments and discussion changes you wish to make via Issues.\nFeel free to propose changes by creating Pull Requests. If you don’t have write access, editing a file will create a Fork of this project for you to save your proposed changes to. Submitting a change to a file will write it to a new Branch in your Fork, so you can send a Pull Request."
  },
  {
    "objectID": "CONTRIBUTING.html#how-to-contribute",
    "href": "CONTRIBUTING.html#how-to-contribute",
    "title": "Contributing",
    "section": "",
    "text": "When contributing, post comments and discussion changes you wish to make via Issues.\nFeel free to propose changes by creating Pull Requests. If you don’t have write access, editing a file will create a Fork of this project for you to save your proposed changes to. Submitting a change to a file will write it to a new Branch in your Fork, so you can send a Pull Request."
  },
  {
    "objectID": "project-content/meeting-minutes.html",
    "href": "project-content/meeting-minutes.html",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "This document summarizes the meetings of the workstream\n\n\n\n\n\nIntroductions\nDiscussions on problem statement and possible solutions\nIdeas on how to go forward\nWrap up and immediate next steps\n\n\n\n\n\nGroup met with context that reproducibility is poor in price statistics research and we should improve it. The group then decided how to tackle this problem and how to properly scope the objectives to make things achievable. Main challenges currently faced in the discipline included:\n\nReserach is becoming increasingly empirical, hence we need processes to work with data and code to make the research more easily reproducible\nThere are no agreed upon benchmark datasets per se in the discipline to test methods on\nThe discipline will not own most of the datasets as most research is done on either confidential internal datasets owned by NSOs or on proprietary (purchased datasets). The open datasets that exist are not organized or cohesively documented/available.\nThe reason why reproducibility is important is not as inherent/widely communicated as it could be, hence any attempts to solve the technical and processes aspects needs to include this aspect in the communication.\n\nThe group touched on goals to solve these aspects and on processes that can be set up to incentivize reproducibility, including by lowering the complexity and creating an easy ‘on ramp’ to making projects more reproducible (including by cataloging open datasets and showcasing how code can be made reproducible), coordinating with the two bi-annual confernece to recommend reproducibility be part of the paper submission process, embedding metadata standards into the data availbile to make it easier and more standardized when various benchmark datasets are used to evaluate a specific method, etc.\nThe data catalogue for open datasets was seen as a major deliverable to the discipline, and one that needed to be phased. In other words we can set up a ‘proof of concept’ or interim catalogue in a simple way to demonstrate the use case, and later transition to a fuller and more comprehensive catalogue with more resources and infrastructure, potentially hosted by the UN Global Platform. Showcasing the interim solution and broadening the adoption to beyond just price statistics would help make this business case.\nThe outcome of the discussion resolved to target two main deliverables: developing the proof of concept data catalogue, and writing guidance (for instance in the form of a white paper) on how to make projects more reproducible. This target scope was later summarized through our project charter.\n\n\n\n\n\n\n\n\nIntro and discussion on timelines and scope. Confirm cadence of meetings\nQuick discussion on how to track PM activity. We could use something like GitHub, the UN wiki, or some other option\nDiscussion on Objective A. We have two options (.Stat suite and data contract catalogue) for interim data catalogue.\nDiscussion on Objective B. Can we adopt any best practices from the Turning Way and their template repo for price statistics?\nRoundtable\n\n\n\n\n\nGroup discussed objectives for the 2025 CPI Expert Group meeting. It was decided to focus on an interim data catalogue and provide interim guidance at the conference, with the fuller guidance to be developed over the next year.\nGitHub projects was agreed upon as the structure for PM activity\nThe draft data catalogue idea was given the green light to flush out further as our likely implementation of the interim version. Metadata strandards should be implemented but it would be hard to use a platform where we don’t own the dataset (such as dominik’s data).\nGuidelines for how to develop reproducible research through git was seen as a good way, with the idea that the guidance we would produce would (a) provide the target state to aim for and (b) summarize maturity levels (similar to RAP maturity levels) that showcase how researchers can start easily and progress over time.\n\n\n\n\n\n\n\n\nDiscussion on the proposed project charter for the project (since merged into main)\nUpdate on the 2 repositories for the project (project repo and interim data catalogue repo) and the mocked up project management approach\nRoundtable\n\n\n\n\n\nTeam discussed scope for 2025 CPI EG presentation. The project plan outlined two milestones, one on data catalogue and one on guidance will be the target. The project structure and use of the GitHub project was summarized.\nThe use of the two repos was discussed. The reproducibility-project repo will host the guidance we develop in whatever format we decide (ideas shared including using quarto to write reproducible papers, or a quarto static site if we will more tend towards creating guidance). price-stats-data-catalogue will host the interim open data catalogue.\nScope of the data contract was firmed up - we could aim to catalogue input datasets that are used to create some experimental indices and version the output datasets (that may be price indices or other artifacts) as part of the repository on github (such as by saving them in data folder and formatting them in tidy data format).\nUse of tools like Zenodo was discussed and will be investigated to mint DOIs - ticket #3\n\n\n\n\n\n\n\n\nDiscussion on format for meeting minutes and how to review/approve the minutes each meeting.\nHow to track materials related to reproducibility but are just references to others’ material (not the overall guidance we will provide).\nWhat format is seen as the best way to deliver guidance on reproducibility? It is best to decide an applicable format and stick with it. Options include writing a paper, using a static site, or using the wiki\n\n\n\n\n\nThe team agreed on keeping notes in this meeting-minutes.md file. The team also agreed to the process:\n\nThe note taker would summarize the meeting and would draft a branch and prepare a Pull Request for the team to review the meeting minutes at the next meeting.\nAt the start of the next meeting, the team would review the PR, would commit any changes/fixes needed. and squash and commit the PR into the main branch to approve the minutes.\n\nThe team agreed to track other materials in other-open-materials.md for now\nThe team discussed the means of how guidance will be provided at the end of the day as a lot of material could be included based on the goals in Objective B of our project. Some possible options:\n\na paper (similar to say the FAIR paper on software to be presented at say 2026 Ottawa Group conference) as the main document for guidance and other materials as supporting (like the catalogue). This could use the quarto manuscript format for example.\na static site (again say a quarto one like this one) as the main means of sharing guidance, but also a short paper for the 2026 OG conference as an offline guide\na set of wiki pages similar to other content made by the Task Team\n\nThe team agreed that a static site (the quarto option) is likely the most appropriate as the site can be expanded and maintained as appropriate, a presentation with a link to the site could be provided at conferences.\nThe team discussed planning for the next 1.5 months. The project roadmap outlines the target timelines. As several issues remain unassigned, the team are encouraged to sign up based on bandwidth.\nRoundtable discussion included:\n\nUpdate on Zenodo - which is an option for uploading and citing data (detail in issue 3). There isn’t a process yet identified for datasets that are not owned by the community and where the owner does not upload it to a repository that mints a DOI.\nUpdate on citing data in PriceIndices R package as it has a DOI. Ability to extract the data without installing the package has to be confirmed.\n\n\n\n\n\n\n\n\n\nReview mock up quarto site for the project, as well as contributing and code of conduct\nReview ongoing work prior to the 2025 CPI Expert Group\nRoundtable\n\n\n\n\n\nTeam discussed the mock up site structure with a home and two main sections, one on how to make your projects reproducible by publishing your code and on using open data.\nThe team also discussed the basic contributing guide and code of conduct.\nThe deliverables to be presented at the 2025 CPI Expert Group were discussed as per the view in the project roadmap.\nAn approach on how to import data from a package was discussed. For instance, how should researchers use data from an R package (such as the PriceIndices package, which has datasets we would like to make list in the interim catalogue) and they wanted to import the data and use it in Python? The team discussed on a phased approach: guidance on how to download the dataset and use it in python from the R ecosystem will be proposed; a longer term approach could be to work with dataset owners to get them to publish it on a repository like Zenodo.\nSupport by the UN Global Platform team for the project was also discussed\n\n\n\n\n\n\n\n\nReview of skeleton data catalogue\nReview of the changes to the project site, specifically how to cite code and how to cite data sections\nReview roadmap and discussion of topics left to finish in this phase of the project\n\n\n\n\n\nThe team discussed the skeleton of the proof of concecept data catalogue. The technical process to register new datasets is basically to (1) draft a new yaml file in the datasets/ folder using the datacontrac.cli specifications, and then (2) when the PR is accepted (after releveant review) and merged with the main branch, the runner will rerender the catalogue and the dataset will show up.\nThe team discussed next steps. The dataset in #6 is still the third we’d want for presentation at CPI EG.\nThere is a need to differentiate open versus proprietary but popular datasets. Open datasets will be the focus for now with potential for expansion after the conference.\nThe team discussed how to cite datasets and how to cite code topics, and based on the example by the recent Baker et al (2022) FAIR principles for software paper, we decided to go with a nuanced recomemndation for now:\n\nif data or code that a research uses exists should be included in the bibliography\ndata or code that is created as part of the paper should be (ideally published to something that mints a DOI) but the links to the dataset or code are included at the end of the paper under “Data availability” and “Code availability” headers.\nThe idea of topics to discuss after the confernece was also brought up - the process of creating synthetic datasets.\n\nTo support researchers to structure their code, the team also discussed and endoresed recommendign a template RAP."
  },
  {
    "objectID": "project-content/meeting-minutes.html#kick-off",
    "href": "project-content/meeting-minutes.html#kick-off",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Introductions\nDiscussions on problem statement and possible solutions\nIdeas on how to go forward\nWrap up and immediate next steps\n\n\n\n\n\nGroup met with context that reproducibility is poor in price statistics research and we should improve it. The group then decided how to tackle this problem and how to properly scope the objectives to make things achievable. Main challenges currently faced in the discipline included:\n\nReserach is becoming increasingly empirical, hence we need processes to work with data and code to make the research more easily reproducible\nThere are no agreed upon benchmark datasets per se in the discipline to test methods on\nThe discipline will not own most of the datasets as most research is done on either confidential internal datasets owned by NSOs or on proprietary (purchased datasets). The open datasets that exist are not organized or cohesively documented/available.\nThe reason why reproducibility is important is not as inherent/widely communicated as it could be, hence any attempts to solve the technical and processes aspects needs to include this aspect in the communication.\n\nThe group touched on goals to solve these aspects and on processes that can be set up to incentivize reproducibility, including by lowering the complexity and creating an easy ‘on ramp’ to making projects more reproducible (including by cataloging open datasets and showcasing how code can be made reproducible), coordinating with the two bi-annual confernece to recommend reproducibility be part of the paper submission process, embedding metadata standards into the data availbile to make it easier and more standardized when various benchmark datasets are used to evaluate a specific method, etc.\nThe data catalogue for open datasets was seen as a major deliverable to the discipline, and one that needed to be phased. In other words we can set up a ‘proof of concept’ or interim catalogue in a simple way to demonstrate the use case, and later transition to a fuller and more comprehensive catalogue with more resources and infrastructure, potentially hosted by the UN Global Platform. Showcasing the interim solution and broadening the adoption to beyond just price statistics would help make this business case.\nThe outcome of the discussion resolved to target two main deliverables: developing the proof of concept data catalogue, and writing guidance (for instance in the form of a white paper) on how to make projects more reproducible. This target scope was later summarized through our project charter."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section",
    "href": "project-content/meeting-minutes.html#section",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Intro and discussion on timelines and scope. Confirm cadence of meetings\nQuick discussion on how to track PM activity. We could use something like GitHub, the UN wiki, or some other option\nDiscussion on Objective A. We have two options (.Stat suite and data contract catalogue) for interim data catalogue.\nDiscussion on Objective B. Can we adopt any best practices from the Turning Way and their template repo for price statistics?\nRoundtable\n\n\n\n\n\nGroup discussed objectives for the 2025 CPI Expert Group meeting. It was decided to focus on an interim data catalogue and provide interim guidance at the conference, with the fuller guidance to be developed over the next year.\nGitHub projects was agreed upon as the structure for PM activity\nThe draft data catalogue idea was given the green light to flush out further as our likely implementation of the interim version. Metadata strandards should be implemented but it would be hard to use a platform where we don’t own the dataset (such as dominik’s data).\nGuidelines for how to develop reproducible research through git was seen as a good way, with the idea that the guidance we would produce would (a) provide the target state to aim for and (b) summarize maturity levels (similar to RAP maturity levels) that showcase how researchers can start easily and progress over time."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-1",
    "href": "project-content/meeting-minutes.html#section-1",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Discussion on the proposed project charter for the project (since merged into main)\nUpdate on the 2 repositories for the project (project repo and interim data catalogue repo) and the mocked up project management approach\nRoundtable\n\n\n\n\n\nTeam discussed scope for 2025 CPI EG presentation. The project plan outlined two milestones, one on data catalogue and one on guidance will be the target. The project structure and use of the GitHub project was summarized.\nThe use of the two repos was discussed. The reproducibility-project repo will host the guidance we develop in whatever format we decide (ideas shared including using quarto to write reproducible papers, or a quarto static site if we will more tend towards creating guidance). price-stats-data-catalogue will host the interim open data catalogue.\nScope of the data contract was firmed up - we could aim to catalogue input datasets that are used to create some experimental indices and version the output datasets (that may be price indices or other artifacts) as part of the repository on github (such as by saving them in data folder and formatting them in tidy data format).\nUse of tools like Zenodo was discussed and will be investigated to mint DOIs - ticket #3"
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-2",
    "href": "project-content/meeting-minutes.html#section-2",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Discussion on format for meeting minutes and how to review/approve the minutes each meeting.\nHow to track materials related to reproducibility but are just references to others’ material (not the overall guidance we will provide).\nWhat format is seen as the best way to deliver guidance on reproducibility? It is best to decide an applicable format and stick with it. Options include writing a paper, using a static site, or using the wiki\n\n\n\n\n\nThe team agreed on keeping notes in this meeting-minutes.md file. The team also agreed to the process:\n\nThe note taker would summarize the meeting and would draft a branch and prepare a Pull Request for the team to review the meeting minutes at the next meeting.\nAt the start of the next meeting, the team would review the PR, would commit any changes/fixes needed. and squash and commit the PR into the main branch to approve the minutes.\n\nThe team agreed to track other materials in other-open-materials.md for now\nThe team discussed the means of how guidance will be provided at the end of the day as a lot of material could be included based on the goals in Objective B of our project. Some possible options:\n\na paper (similar to say the FAIR paper on software to be presented at say 2026 Ottawa Group conference) as the main document for guidance and other materials as supporting (like the catalogue). This could use the quarto manuscript format for example.\na static site (again say a quarto one like this one) as the main means of sharing guidance, but also a short paper for the 2026 OG conference as an offline guide\na set of wiki pages similar to other content made by the Task Team\n\nThe team agreed that a static site (the quarto option) is likely the most appropriate as the site can be expanded and maintained as appropriate, a presentation with a link to the site could be provided at conferences.\nThe team discussed planning for the next 1.5 months. The project roadmap outlines the target timelines. As several issues remain unassigned, the team are encouraged to sign up based on bandwidth.\nRoundtable discussion included:\n\nUpdate on Zenodo - which is an option for uploading and citing data (detail in issue 3). There isn’t a process yet identified for datasets that are not owned by the community and where the owner does not upload it to a repository that mints a DOI.\nUpdate on citing data in PriceIndices R package as it has a DOI. Ability to extract the data without installing the package has to be confirmed."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-3",
    "href": "project-content/meeting-minutes.html#section-3",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Review mock up quarto site for the project, as well as contributing and code of conduct\nReview ongoing work prior to the 2025 CPI Expert Group\nRoundtable\n\n\n\n\n\nTeam discussed the mock up site structure with a home and two main sections, one on how to make your projects reproducible by publishing your code and on using open data.\nThe team also discussed the basic contributing guide and code of conduct.\nThe deliverables to be presented at the 2025 CPI Expert Group were discussed as per the view in the project roadmap.\nAn approach on how to import data from a package was discussed. For instance, how should researchers use data from an R package (such as the PriceIndices package, which has datasets we would like to make list in the interim catalogue) and they wanted to import the data and use it in Python? The team discussed on a phased approach: guidance on how to download the dataset and use it in python from the R ecosystem will be proposed; a longer term approach could be to work with dataset owners to get them to publish it on a repository like Zenodo.\nSupport by the UN Global Platform team for the project was also discussed"
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-4",
    "href": "project-content/meeting-minutes.html#section-4",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Review of skeleton data catalogue\nReview of the changes to the project site, specifically how to cite code and how to cite data sections\nReview roadmap and discussion of topics left to finish in this phase of the project\n\n\n\n\n\nThe team discussed the skeleton of the proof of concecept data catalogue. The technical process to register new datasets is basically to (1) draft a new yaml file in the datasets/ folder using the datacontrac.cli specifications, and then (2) when the PR is accepted (after releveant review) and merged with the main branch, the runner will rerender the catalogue and the dataset will show up.\nThe team discussed next steps. The dataset in #6 is still the third we’d want for presentation at CPI EG.\nThere is a need to differentiate open versus proprietary but popular datasets. Open datasets will be the focus for now with potential for expansion after the conference.\nThe team discussed how to cite datasets and how to cite code topics, and based on the example by the recent Baker et al (2022) FAIR principles for software paper, we decided to go with a nuanced recomemndation for now:\n\nif data or code that a research uses exists should be included in the bibliography\ndata or code that is created as part of the paper should be (ideally published to something that mints a DOI) but the links to the dataset or code are included at the end of the paper under “Data availability” and “Code availability” headers.\nThe idea of topics to discuss after the confernece was also brought up - the process of creating synthetic datasets.\n\nTo support researchers to structure their code, the team also discussed and endoresed recommendign a template RAP."
  },
  {
    "objectID": "project-content/project-charter.html",
    "href": "project-content/project-charter.html",
    "title": "Project Charter",
    "section": "",
    "text": "Research in the price statistics discipline is not as reproducible as we feel it should be. Most researchers utilize proprietary datasets (for instance internal datasets owned by their NSOs as part of their official work, or purchased datasets that require considerable financial investment for others to acquire). Research is also done using software available to researchers in a way that is custom to them and the code and detailed processes are typically not made easily available as part of the research project. This consequence is far from the intention of researchers, but is a result of the challenges to do this within the discipline. Specifically, it is not easy to find or access open datasets that can be used for research purposes. Once data is found, the metadata on each dataset will differ, making it challenging to process and use the data, such as for repeatable research processes. Finally, once a researcher has access to the data, it is not clear how code and processing logic should be shared as part of the research project to make the project reproducible. In other words, the process is not Findable, Accessible, Interoperable, or Reusable (or FAIR).\n\n\n\nThe project aims to simplify this situation in the price statistics discipline by tackling the challenges that researchers face. In other words, the project aims to lower the barrier for reproducibility, making it intuitively easy (with some practice) for researchers to work openly. From an open science point of view, making research more routinely reproducible will help accelerate the pace at which consensus is reached on various topics as results can quickly become replicable and even generalizable.\n\n\n\nAs the project tackles the main challenges facing the discipline, it aims to deliver the following aspects:\n\nObjective A: Developing an interim data catalogue tool for several open datasets and publishing several open datasets as a proof of concept. The idea is to start helping researchers know how to reference open datasets for better replicability within the discipline\n\nA.1. Mock up an interim data catalogue on the UNGP GitLab static site that is easy to read through and understand. The catalogue will be made available on its own GitHub repo.\nA.2. Investigate the applicable metadata for publishing/registering a dataset and outline an ingestion/registration process that can be leveraged to onboard datasets to the interim data catalogue.\nA.3. Coordinate the registering of several open datasets that can act as a pilot for the interim data catalogue.\n\nObjective B: Developing a white paper to outline the why, the what, the where, the when (in the research process) and the how. The idea is to create a high level guide for researchers, and could build on the FAIR or reproducibility literature and apply it to our domain. The paper would have several sub components that should be investigated and discussed, such as:\n\nB.1. Investigate processes around data – such as how to reference the data (internal, public, or synthetic), how to incentivize the use of benchmark datasets for specific tasks in the discipline, how to deal with complex use cases (such as confidentiality or privately owned but widely used data).\nB.2. Investigate processes around code and related objects like clear code documentation – such as where to publish code, what should be included in the repository, how to clearly document the process, etc. If applicable, make template repositories or examples available for the community.\nB.3. Investigate administrative topics that are useful for the discipline, such as how to coordinate with the 2 conferences we attend to make sure that we embed and incentivize the use of the processes we would like to develop.\nB.4. Outlining of the white paper/guidance for the discipline on reproducibility\n\n\n\n\n\nTo manage the project, a GitHub projects is used for coordination and transparency."
  },
  {
    "objectID": "project-content/project-charter.html#project-overview",
    "href": "project-content/project-charter.html#project-overview",
    "title": "Project Charter",
    "section": "",
    "text": "Research in the price statistics discipline is not as reproducible as we feel it should be. Most researchers utilize proprietary datasets (for instance internal datasets owned by their NSOs as part of their official work, or purchased datasets that require considerable financial investment for others to acquire). Research is also done using software available to researchers in a way that is custom to them and the code and detailed processes are typically not made easily available as part of the research project. This consequence is far from the intention of researchers, but is a result of the challenges to do this within the discipline. Specifically, it is not easy to find or access open datasets that can be used for research purposes. Once data is found, the metadata on each dataset will differ, making it challenging to process and use the data, such as for repeatable research processes. Finally, once a researcher has access to the data, it is not clear how code and processing logic should be shared as part of the research project to make the project reproducible. In other words, the process is not Findable, Accessible, Interoperable, or Reusable (or FAIR)."
  },
  {
    "objectID": "project-content/project-charter.html#raison-dêtre-of-the-project",
    "href": "project-content/project-charter.html#raison-dêtre-of-the-project",
    "title": "Project Charter",
    "section": "",
    "text": "The project aims to simplify this situation in the price statistics discipline by tackling the challenges that researchers face. In other words, the project aims to lower the barrier for reproducibility, making it intuitively easy (with some practice) for researchers to work openly. From an open science point of view, making research more routinely reproducible will help accelerate the pace at which consensus is reached on various topics as results can quickly become replicable and even generalizable."
  },
  {
    "objectID": "project-content/project-charter.html#expected-outcomes-in-a-little-more-detail",
    "href": "project-content/project-charter.html#expected-outcomes-in-a-little-more-detail",
    "title": "Project Charter",
    "section": "",
    "text": "As the project tackles the main challenges facing the discipline, it aims to deliver the following aspects:\n\nObjective A: Developing an interim data catalogue tool for several open datasets and publishing several open datasets as a proof of concept. The idea is to start helping researchers know how to reference open datasets for better replicability within the discipline\n\nA.1. Mock up an interim data catalogue on the UNGP GitLab static site that is easy to read through and understand. The catalogue will be made available on its own GitHub repo.\nA.2. Investigate the applicable metadata for publishing/registering a dataset and outline an ingestion/registration process that can be leveraged to onboard datasets to the interim data catalogue.\nA.3. Coordinate the registering of several open datasets that can act as a pilot for the interim data catalogue.\n\nObjective B: Developing a white paper to outline the why, the what, the where, the when (in the research process) and the how. The idea is to create a high level guide for researchers, and could build on the FAIR or reproducibility literature and apply it to our domain. The paper would have several sub components that should be investigated and discussed, such as:\n\nB.1. Investigate processes around data – such as how to reference the data (internal, public, or synthetic), how to incentivize the use of benchmark datasets for specific tasks in the discipline, how to deal with complex use cases (such as confidentiality or privately owned but widely used data).\nB.2. Investigate processes around code and related objects like clear code documentation – such as where to publish code, what should be included in the repository, how to clearly document the process, etc. If applicable, make template repositories or examples available for the community.\nB.3. Investigate administrative topics that are useful for the discipline, such as how to coordinate with the 2 conferences we attend to make sure that we embed and incentivize the use of the processes we would like to develop.\nB.4. Outlining of the white paper/guidance for the discipline on reproducibility"
  },
  {
    "objectID": "project-content/project-charter.html#project-management",
    "href": "project-content/project-charter.html#project-management",
    "title": "Project Charter",
    "section": "",
    "text": "To manage the project, a GitHub projects is used for coordination and transparency."
  },
  {
    "objectID": "docs/faq.html",
    "href": "docs/faq.html",
    "title": "Frequently asked questions",
    "section": "",
    "text": "Help us fill this page!\n\n\n\nHave an idea for a common topic to add to this FAQ? Give us a shout! Submit an issue to the project and tag the the issue using the “question” label.\n\n\nHow do I register a dataset to the catalogue (or recommend a dataset be registered)?\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/datasets-guidance/catalogue.html",
    "href": "docs/datasets-guidance/catalogue.html",
    "title": "Catalogue of open data in price statistics",
    "section": "",
    "text": "There are two key challenges in the price statistics discipline when it comes to using open data. On the one hand, there are few open data available as most datasets of sufficient size and quality are proprietary. On the other hand, it is hard to find out what datasets do exist as there is not discipline specific listing. As most researchers are employed at organizations (such as National Statistical Organizations or NSOs) naturally have access to internal datasets and often are under pressure to answer questions relating them, it is only natural to default to proprietary over datasets. To help alleviate this, the project has developed a basic discipline specific data catalogue."
  },
  {
    "objectID": "docs/datasets-guidance/catalogue.html#price-statistics-open-data-catalogue",
    "href": "docs/datasets-guidance/catalogue.html#price-statistics-open-data-catalogue",
    "title": "Catalogue of open data in price statistics",
    "section": "Price Statistics Open Data Catalogue",
    "text": "Price Statistics Open Data Catalogue\nThe idea of this data catalogue is to list open datasets available and often used within price statistics discipline. It describes each dataset in a structured and detailed manner, allowing researchers to search for a dataset that may fit their needs, understand the options, navigate to the site of the data provider that hosts that dataset, and then use it for their research.\n\n\n\nFigure 1: Basic idea for what the data catalogue does\n\n\n\nWhat the catalogue does\nThe purpose of the catalogue is to be a very simple listing of open datasets applicable for research purposes. It is searchable according to standard data types (such as scanner data), and describes the metadata necessary for researchers to understand and select the dataset most applicable to them.\n\n\nWhat the catalogue does not do\nThe catalogue does not store the dataset itself but simply describes it in detail. The catalogue does provide a simple interface to common researcher questions, such as how to cite the dataset, limitations to its use, etc.\n\n\n\n\n\n\nThis is an interim catalogue only!\n\n\n\nThis will very likely not be the long-term stable data catalogue the discpline. The idea however it to start with this interim (and very simple open-source) catalogue, while the project investigates a more viable longer term solution."
  },
  {
    "objectID": "docs/reproducibility-guidance/citing-code.html",
    "href": "docs/reproducibility-guidance/citing-code.html",
    "title": "How to cite code",
    "section": "",
    "text": "Citing software is historically not something that researchers have done. Code can often be hard to cite and the software used to derive the results of a paper was usually seen as a “detail” that is not worth mentioning in the body of paper. (Perhaps to avoid too many questions about how, exactly, the results in the paper were derived.) The last decade has seen a move towards more reproducible research in economics—mainstream journals require code and usually data for quantitative papers—and the proliferation of open-source research software has made it easier to reliably cite the software used for research.\nCiting code is particularly important in the area of price statistics to facilitate reproducible research. There are a great many price index methods, often with fiddly variations, used by different researchers and it is important to know the exact implementation used to generate a price index in order to reproduce the construction of that index. Research for price statistics is also usually done by government agencies; transparency about the code to generate the results that inform the methods used by government agencies is important to maintain the trust in official statistics.\nFor further reading, the Turing Way provides a good overview for citing code in research and represents the current state of the world of open-source research software."
  },
  {
    "objectID": "docs/reproducibility-guidance/citing-code.html#citing-research-software",
    "href": "docs/reproducibility-guidance/citing-code.html#citing-research-software",
    "title": "How to cite code",
    "section": "Citing research software",
    "text": "Citing research software\nUnlike academic papers, there is no one way to cite a piece of software. How to cite a piece of software usually depends on how easy the author makes it. For example, R packages are easy to cite within R using the citation() function.\n\ncitation(\"IndexNumR\")\n\nTo cite package 'IndexNumR' in publications use:\n\n  White G (2023). _IndexNumR: Index Number Calculation_. R package\n  version 0.6.0, &lt;https://github.com/grahamjwhite/IndexNumR&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {IndexNumR: Index Number Calculation},\n    author = {Graham White},\n    year = {2023},\n    note = {R package version 0.6.0},\n    url = {https://github.com/grahamjwhite/IndexNumR},\n  }\n\nATTENTION: This citation information has been auto-generated from the\npackage DESCRIPTION file and may need manual editing, see\n'help(\"citation\")'.\n\n\nModern R packages tend to have their own website with this information on display and all CRAN packages get a DOI to facilitate referencing the use of R packages. Things are less standardized in the Python ecosystem, but the same ideas apply to make projects citeable (e.g., pandas).\nAnother way to cite a piece of research software is to the cite the paper introducing this software, usually in a journal like the Journal of Open Source Software, the Journal of Statistical Software, or the R Journal.\n@article{RJ-2021-038,\n  author = {Saavedra-Nieves, Alejandro and Saavedra-Nieves, Paula},\n  title = {IndexNumber: An R Package for Measuring the Evolution of Magnitudes},\n  journal = {The R Journal},\n  year = {2021},\n  note = {https://rjournal.github.io/},\n  volume = {13},\n  issue = {1},\n  issn = {2073-4859},\n  pages = {253-275}\n}\nFinally, for code that is not in a mainstream repository like CRAN, or does not have a website with citation information, citation information can sometimes be found in the source code. Github helps to find the citation information for a package and displays a button to cite the repository."
  },
  {
    "objectID": "docs/reproducibility-guidance/citing-code.html#making-software-citable",
    "href": "docs/reproducibility-guidance/citing-code.html#making-software-citable",
    "title": "How to cite code",
    "section": "Making software citable",
    "text": "Making software citable\nGiven the variety of ways to cite research software, the key to making it citeable is making it easy to generate a reference for that software. For R packages this happens automatically if the package is available on CRAN; for Python packages (and R packages not on CRAN), a service like Zenodo can be used to get a DOI to facilitate referencing the software. Although consumers of software tend not to get it directly from a source-code repository, citation metadata can be added to github repositories to make them more citeable."
  },
  {
    "objectID": "docs/reproducibility-guidance/citing-code.html#example",
    "href": "docs/reproducibility-guidance/citing-code.html#example",
    "title": "How to cite code",
    "section": "Example",
    "text": "Example\nThe {piar} R package is an example of a piece of software for making price indexes that is highly citeable.\n\ncitation(\"piar\")\n\nTo cite package 'piar' in publications use:\n\n  Martin S (2024). \"piar: Price Index Aggregation R.\" _Journal of Open\n  Source Software_, *9*(101), 6781. doi:10.21105/joss.06781\n  &lt;https://doi.org/10.21105/joss.06781&gt;.\n\n  Martin S (2024). _piar: Price Index Aggregation_.\n  doi:10.5281/zenodo.10110046\n  &lt;https://doi.org/10.5281/zenodo.10110046&gt;, R package version 0.8.1,\n  &lt;https://cran.r-project.org/package=piar&gt;.\n\nTo see these entries in BibTeX format, use 'print(&lt;citation&gt;,\nbibtex=TRUE)', 'toBibtex(.)', or set\n'options(citation.bibtex.max=999)'.\n\n\nThis information is displayed on the project website and CRAN. The citation information is contained in the source code, and consequently displayed by github, and the readme for the project is adorned with badges giving citation information. The goal is to have citation information available at each entry point at which a prospective user may first engage with {piar} and to have it be easy to add to a reference list."
  },
  {
    "objectID": "docs/reproducibility-guidance/intro.html",
    "href": "docs/reproducibility-guidance/intro.html",
    "title": "Why and how to approach reproducibility",
    "section": "",
    "text": "Reproducible research projects set out to make all elements of the research project available for others openly so that the project can be reproduced. Specifically, it means that the research compendium, which are these elements assembled together in a logical and clear fashion, are published alongside the research paper. The practice creating and publishing a research compendium may involve some technical skills, however it will help the research team leading the project, other researchers, and the discipline as a whole. It will also help publish official statistics with the methods being researched, as the Reproducible Analytical Pipelines (RAP) principles applied to reproducible and technically mature production processes leverage the very same technical best practices. Indeed, RAP and research compedia are conceptually the same things – the application of technical processes for reproducibility and process robustness – its just one focuses more on research and the other on production of official statistics."
  },
  {
    "objectID": "docs/reproducibility-guidance/intro.html#structuring-the-research-compendium",
    "href": "docs/reproducibility-guidance/intro.html#structuring-the-research-compendium",
    "title": "Why and how to approach reproducibility",
    "section": "Structuring the research compendium",
    "text": "Structuring the research compendium\nIn a nutshell, a compendium includes all research objects necessary to reproduce the research. In a technical sense, these are often git repositories (in say GitHub) that include a structure similar to the one in Figure 1 below.\n\n\n\nFigure 1: Exemplar price index pipeline.\n\n\nSpecifically:\n\nA data folder stores the raw dataset used for the research project. Ideally the researcher uses an open dataset (which will make the whole process reproducible), but they may also use a proprietary dataset. Note the dataset itself should not be version controlled with the repository but the ability to save the dataset in this folder means that others can download the repository, save the dataset in the same folder, and replicate the results. In a technical sense, the data itself is ignored by making sure that it is listed in the .gitignore file.\nA functions (or code) folder that does the research aspects. This could include the code to clean and prepare the raw dataset for research purposes, the code to create the processing and research experients, as well as notebooks where the data is explored and various aspects that feed the research paper are generated.\nA folder for the output data. This data can be versioned (if it is not sensitive) with the repository and allows researchers to replicate the process. Note, if the output data can be used for research in its own right, it may be appropriate to register this dataset on a public repository (such as Zenodo) that mints a DOI.\nA folder for the docs that explain how the various aspects of the research come together. This folder stores project documentation or the project design, however code should also be documented well.\nA licence is included to tell users how they can use the code.\n.gitignore file ignores various files (such as datasets) from being version controlled\nA file to recreate the environment so that the code can be run as it ran on the machine of the researcher.\nFinally, a README to introduce the project when someone navigates to the project\n\nTo showcase an exemplar for price statistics, we created a mock-up price index pipeline that researchers can explore."
  }
]